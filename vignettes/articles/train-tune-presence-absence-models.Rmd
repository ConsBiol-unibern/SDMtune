---
title: "Train presence absence models"
bibliography: ../SDMtune.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "#>",
                      fig.align = "center")
options(knitr.table.format = "html")
```

```{r load-data, echo=FALSE, message=FALSE}
library(SDMtune)
files <- list.files(path = file.path(system.file(package = "dismo"), "ex"), 
                    pattern = "grd", 
                    full.names = TRUE)
predictors <- terra::rast(files)
```

## Intro

All the previous articles are based on presence only methods, in this article you will learn how to train a presence absence model. The following examples are based on the Artificial Neural Networks method [@Venables2002], but you can adapt the code for any of the other supported methods.

## Prepare the data for the analysis

We use the first 8 environmental variables and the same `virtualSp()` dataset selecting the absence instead of the background locations.

```{r prepare-data, eval=FALSE}
p_coords <- virtualSp$presence
a_coords <- virtualSp$absence

data <- prepareSWD(species = "Virtual species", 
                   p = p_coords, 
                   a = a_coords, 
                   env = predictors[[1:8]])

data
```
```{r prepare-data-output, echo=FALSE}
p_coords <- virtualSp$presence
a_coords <- virtualSp$absence

data <- prepareSWD(species = "Virtual species", 
                   p = p_coords, 
                   a = a_coords, 
                   env = predictors[[1:8]],
                   verbose = FALSE)

data
```

There are `r sum(data@pa == 1)` presence and `r sum(data@pa == 0)` absence locations.

For the model evaluation we will create a training and testing datasets, holding apart 20% of the data:

```{r train-test}
library(zeallot)
c(train, test) %<-% trainValTest(data, 
                                 test = 0.2, 
                                 seed = 25)
```

At this point we have `r nrow(train@data)` training and `r nrow(test@data)` testing locations. We create a 4-folds partition to run cross validation:

```{r folds}
folds <- randomFolds(train, 
                     k = 4, 
                     seed = 25)
```

## Train the model

We first train the model with default settings and 10 neurons:

```{r ann}
set.seed(25)
model <- train("ANN", 
               data = train, 
               size = 10, 
               folds = folds)
model
```

Let's check the training and testing AUC:

```{r auc}
auc(model)
auc(model, test = TRUE)
```

## Tune model hyperparameters

To check which hyperparameters can be tuned we use the function `getTunableArgs()` function:

```{r get-tunable-args}
getTunableArgs(model)
```

We use the function `optimizeModel()` to tune the hyperparameters:

```{r optimize-model, eval=FALSE}
h <- list(size = 10:50, 
          decay = c(0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5),
          maxit = c(50, 100, 300, 500))

om <- optimizeModel(model, 
                    hypers = h, 
                    metric = "auc", 
                    seed = 25)
```
```{r optimize model output, echo=FALSE}
h <- list(size = 10:50, 
          decay = c(0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5),
          maxit = c(50, 100, 300, 500))

om <- optimizeModel(model, 
                    hypers = h, 
                    metric = "auc", 
                    seed = 25,
                    interactive = FALSE,
                    progress = FALSE)
```

The best model is:

```{r best-model, eval=FALSE}
best_model <- om@models[[1]]
om@results[1, ]
```
```{r best-model-output, echo=FALSE}
best_model <- om@models[[1]]
kableExtra::kable(om@results[1, ]) |> 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"),
                            position = "center",
                            full_width = FALSE)
```

The validation AUC increased from `r auc(model, test = TRUE)` of the default models to `r om@results[1, 6]` of the optimized one.

## Evaluate the final model

We now train a model with the same configuration as found by the function `optimizeModel()` without cross validation (i.e. using all presence and background locations) and we evaluate it using the held apart testing dataset:

```{r evaluate-final-model}
set.seed(25)
final_model <- combineCV(om@models[[1]])

plotROC(final_model, 
        test = test)
```

## Conclusion

In this tutorial you have learned a general way to train, evaluate and tune model using Artificial Neural Network, but you can apply the same workflow to other methods.

### References
