---
title: "Evaluation strategies"
bibliography: ../SDMtune.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(knitr.table.format = "html")
```

```{r load data, echo=FALSE, message=FALSE}
library(kableExtra)
data <- SDMtune:::t
files <- list.files(path = file.path(system.file(package = "dismo"), "ex"), pattern = "grd", full.names = T)
predictors <- raster::stack(files)
```

## Intro

In the previous articles you have learned how to [prepare the data](./prepare-data.html) for the analysis, how to [train a model](./train-model.html), how to [make predictions](./make-predictions.html) and how to [evaluate a model](./evaluate-model.html) using **SDMtune**. In this article you will learn different evaluation strategies to achieve a better estimate of the model performance.

## Training and testing datasets

First we load the **SDMtune** package:

```{r load SDMtune, warning=FALSE}
library(SDMtune)
```

It's always a good practice to split the species locations into two parts and use one part to train the model and the remaining part to evaluate it. We can use the `trainValTest()` function for this purpose. Let's say we want to use 80\% of the species locations to train our model and 20\% as testing dataset to evaluate it:
```{r train test}
library(zeallot)  # For unpacking assignment
c(train, test) %<-% trainValTest(data, test = 0.2, only_presence = TRUE, seed = 25)
maxnet_model <- train("Maxnet", data = train)
```

The `only_presence` argument is used to split only the presence and not the background locations. We can now evaluate the model using the testing dataset that has not been used to train the model:
```{r evaluate test}
cat("Training auc: ", auc(maxnet_model))
cat("Testing auc: ", auc(maxnet_model, test = test))
```

We can plot the ROC curve for both, training and testing datasets, with:
```{r plot AUC train/test}
plotROC(maxnet_model, test = test)
```

This approach is valid when we have a large dataset. In our case, with only `r nrow(train@data[train@pa == 1, ])` observations, the evaluation depends strongly on how we split our presence locations. Let's run a small experiment in which we perform different train/test splits and we compute the AUC:
```{r experiment}
output <- data.frame(matrix(NA, nrow = 10, ncol = 3)) # Create an empty data.frame
colnames(output) <- c("seed", "trainAUC", "testAUC")
set.seed(25)
seeds <- sample.int(1000, 10) # Create 10 different random seeds

for (i in 1:length(seeds)) { # Loop through the seeds
  c(train, test) %<-% trainValTest(data, test = 0.2, seed = seeds[i], only_presence = TRUE) # Make the train/test split
  m <- train("Maxnet", data = train) # train the model
  # Populate the output data.frame
  output[i, 1] <- seeds[i]
  output[i, 2] <- auc(m)
  output[i, 3] <- auc(m, test = test)
}
```

The testing AUC varies from `r min(output[, 3])` to `r max(output[, 3])`. When we have to deal with a small dataset a better approach is the cross validation.
```{r table}
# Print the output
output
range(output[, 3])
```

## Cross validation

To perform a cross validation in **SDMtune** we have to pass the `fold` argument to the `train()` function. First we have to create the folds. There are several way to create them, here we explain how to make a random partition of 4 folds using the function `randomFolds()`:
```{r random folds}
folds <- randomFolds(data, k = 4, only_presence = TRUE, seed = 25)
```

The output of the function is a list containing two matrices, the first for the training and the second for the testing locations. Each column of one matrix represents a fold with `TRUE` for the locations included in and `FALSE` excluded from the partition.

Let's perform a 4 fold cross validation using the **Maxnet** method (note that we use the full dataset):
```{r cv, eval=FALSE}
cv_model <- train("Maxnet", data = data, folds = folds)
cv_model
```
```{r cv bis, echo=FALSE}
cv_model <- SDMtune:::bm_maxnet_cv
cv_model
```

The output in this case is an `SDMmodelCV()` object. It contains the four trained models in the `models` slot and the fold partitions in the `folds` slot. We can compute the AUC of a `SDMmodelCV()` object using:
```{r}
cat("Training AUC: ", auc(cv_model))
cat("Testing AUC: ", auc(cv_model, test = TRUE))
```

this returns the AUC value averaged across the four different models.

### Try yourself

Repeat the analysis using the `default_model` that we created in the [train a model](./model-training.html#train-a-model-with-default-settings) article.

### Spatial Cross Validation

The `train()` function accepts folds created with two other packages:

* `ENMeval` [@Muscarella2014]
* `blockCV` [@Valavi2019]

The function will convert internally the created folds into the correct format for `SDMtune`. These packages have specific function to create folds partitions that are spatially or environmentally independent.

Block partition using the package `ENMeval`:
```{r enmeval block, eval=FALSE}
library(ENMeval)
block_folds <- get.block(occ = data@coords[data@pa == 1, ], bg.coords = data@coords[data@pa == 0, ])
model <- train(method = "Maxnet", data = data, fc = "l", reg = 0.8, folds = block_folds)
```

Checkerboard1 partition using the package `ENMeval`:
```{r enmeval checherboard1, eval=FALSE}
cb_folds <- get.checkerboard1(occ = data@coords[data@pa == 1, ], env = predictors, bg.coords = data@coords[data@pa == 0, ], aggregation.factor = 4)
model <- train(method = "Maxnet", data = data, fc = "l", reg = 0.8, folds = cb_folds)
```


Environmental block using the package `blockCV`:
```{r blockCV, eval=FALSE}
library(blockCV)
library(raster)
# Create spatial points data frame
sp_df <- SpatialPointsDataFrame(data@coords, data = as.data.frame(data@pa), proj4string = crs(predictors))
e_folds <- envBlock(rasterLayer = predictors, speciesData = sp_df, species = "data@pa", k = 4, standardization = "standard", rasterBlock = FALSE, numLimit = 100)
model <- train(method = "Maxnet", data = data, fc = "l", reg = 0.8, folds = e_folds)
```

## Conclusion

In this article you have learned:

* how to split the presence dataset in training and testing folds;
* how to evaluate the model using the held out testing dataset;
* how to create random folds for cross validation;
* how to perform a cross validation;
* how to use other packages to create the folds for the k-fold cross validation;
* how to evaluate an `SDMmodelCV()` object.

In the [next article](./variable-importance.html) you will learn how to display the variable importance and how to plot the response curve.
