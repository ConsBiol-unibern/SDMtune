---
title: "Tune model using AUC"
author: "Sergio Vignali, Arnaud Barras, Veronika Braunisch"
bibliography: ../library.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(knitr.table.format = "html")
```

# Set working environment

Import `SDMsel`:
```{r import}
options(java.parameters = "-Xmx2g" )
library(SDMsel)
```

Load packages used in the vignette:

```{r, message=F}
library(dismo)      # To download data used in the vignette
library(plotly)     # For interactive graphs
library(maps)       # For access useful maps
library(rasterVis)  # To plot raster objects
library(gridExtra)  # To plot multiple ggplot object in one page
```


# Acquire environmental variables and species distribution data

For the analysis we will use the climat data from [WorldClim](http://www.worldclim.org/) version 1.4 [@Hijmans2005] (you can download the new version 2.0 from the previous link) and the ecoregions from [WWF](https://www.worldwildlife.org/publications/terrestrial-ecoregions-of-the-world) [@Olson2001] included in the `dismo` package:

```{r get predictors, fig.align="center"}
files <- list.files(path = paste(system.file(package = 'dismo'), '/ex', sep = ''), pattern = 'grd', full.names = T)
predictors <- stack(files)
```

Plot **bio1** using [gplot](https://www.rdocumentation.org/packages/rasterVis/versions/0.43/topics/gplot-methods) function from `rasterVis` package:

```{r, fig.align="center"}
gplot(predictors$bio1) +
    geom_tile(aes(fill = value)) +
    coord_equal() +
    scale_fill_gradientn(colours = c("#2c7bb6", "#abd9e9", "#ffffbf", "#fdae61", "#d7191c"),
                         na.value = "transparent",
                         name = '°C x 10') +
    labs(title = 'Annual Mean Temperature',
         x = 'longitude',
         y = 'latitude') +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.ticks.x = element_blank(),
          axis.ticks.y = element_blank())
```

There are nine environmental variables:

* Continuous variables
    + **bio1** Annual Mean Temperature
    + **bio5** Max Temperature of Warmest Month
    + **bio6** Min Temperature of Coldest Month
    + **bio7** Temperature Annual Range (bio5-bio6)
    + **bio8** Mean Temperature of Wettest Quarter
    + **bio12** Annual Precipitation
    + **bio16** Precipitation of Wettest Quarter
    + **bio17** Precipitation of Driest Quarter
* Categorical variables
    + **biome** Terrestrial Ecoregions of the World

For demostrating how to use `SDMsel` we download a dataset of [*Vultur gryphus*](https://en.wikipedia.org/wiki/Andean_condor) from  the Global Biodiversity Inventory Facility [GBIF](http://www.gbif.org/) using the [gbif](https://www.rdocumentation.org/packages/dismo/versions/1.1-4/topics/gbif) function of `dismo` package. We download data from the last 3 years.

```{r get presence data}
data <- gbif(genus = 'Vultur', species = 'gryphus*', geo = TRUE, removeZeros = TRUE, args = 'year=2015,2017')
data <- data.frame(x = data$lon, y = data$lat)
cleaned_data <- data[!duplicated(data), ]  # Remove duplicate rows
cleaned_data <- cleaned_data[!is.na(cleaned_data$x) | !is.na(cleaned_data$y), ]  # Remove NA values
```

We remove all but one location per raster cell using the function `thinData`:

```{r}
cleaned_data <- thinData(cleaned_data, predictors)
```


```{r}
cat('Number of observations before cleaning data:', nrow(data),
    '\nNumber of obsevations after cleaning data  :', nrow(cleaned_data))
```

We downloaded `r nrow(data)` presence locations and after removing the duplicates, the NA and all but one location per raster cell we have `r nrow(cleaned_data)` locations for our analysis. We can plot the study area and the cleaned locations:

```{r, fig.align="center"}
ggplot(map_data('world'), aes(long, lat)) +
    geom_polygon(aes(group = group), fill = "grey95", color = "gray40", size = 0.2) +
    geom_jitter(data = cleaned_data, aes(x = x, y = y), color = 'red',
                alpha = 0.4, size = 1) +
    labs(x = "longitude", y = "latitude") +
    theme_minimal() +
    theme(legend.position = "none") +
    coord_fixed() +
    scale_x_continuous(limits = c(-125, -32)) +
    scale_y_continuous(limits = c(-56, 40))
```

In this tutorial we don't check the validity of the locations, instead we assume they are correct and we model the distribution only for demonstration purpose.


# Model Selection

MaxEnt is a Machine Learning (ML) approach for Species Distribution Models and most of the ML methods require to define several **hyperparameters** before training the model. Hyperparameters are parameters whose values are set before the learning process begins and are not estimated from the data. Their best value is unknown for a given model but can be assessd using a tuning precedure. During the tuning procedure several models are trained varying the value of a hyperparameters and the model with the best performance is retained. The performance of a model can be evaluated using various metrics, `SDMsel` provides three: `AUC`, `TSS` (True Skill Statistic) and `AICc`.

## Prepare data for Model Selection

In this vignette we demonstrate how to perform model selction using the `AUC`. In ML part of the data are withheld for testing purpose. A model is trained using the majority of the data and then evaluated using the withheld dataset. During hyperparameter tuning this process is repeated several times and the hyperparameters are tuned to improve the prediction on the test dataset. At the end of the model selection process the test dataset doesn't represent anymore an indipendet sample and then is a good practice to split the data randomly in three parts: **Train**, **Validation** and **Test** datasets [@Russell2013]. The **Train** dataset is used to train the model, the **Validation** dataset to estimate the model performance while tuning the hyperparameters and the **Test** dataset is used at the end of the tuning process to have an unbiased evaluation of the final model. For a good overview of the topic visit the [Jason Brownlee’s](https://machinelearningmastery.com/difference-test-validation-datasets/) thread.  
We create a random train/val/test datasets withholding a 20% sample for validation and a 20% for testing. We set the seed to obtain consistent results between different trails:

```{r split data in train/dev/test dataset}
# The %<-% operator from "zeallot"" package unpacks a list in variables
c(train_coords, val_coords, test_coords) %<-% trainValTest(cleaned_data, val = 0.2, test = 0.2, seed = 2530)
cat('Total presence locations  :', nrow(cleaned_data),
    '\nTotal train locations     :', nrow(train_coords),
    '\nTotal validation locations:', nrow(val_coords),
    '\nTotal test locations      :', nrow(test_coords))
```

Plot **Train**, **Validation** and **Test** datasets:

```{r, fig.align="center"}
base_plot <- ggplot(map_data('world'), aes(long, lat)) +
  geom_polygon(aes(group = group), fill = "grey95", color = "gray40", size = 0.1) +
  labs(x = "longitude", y = "latitude") +
  theme_minimal() +
  theme(legend.position = "none") +
  coord_fixed() +
  scale_x_continuous(limits = c(-125, -32)) +
  scale_y_continuous(limits = c(-56, 40)) +
  theme(plot.title = element_text(hjust = 0.5))

train_plot <- base_plot + geom_jitter(data = train_coords, aes(x = x, y = y), color = 'red', alpha = 0.4, size = 1) + labs(title = 'Train')
val_plot <- base_plot + geom_jitter(data = val_coords, aes(x = x, y = y), color = 'green', alpha = 0.4, size = 1) + labs(title = 'Validation')
test_plot <- base_plot + geom_jitter(data = test_coords, aes(x = x, y = y), color = 'blue', alpha = 0.4, size = 1) + labs(title = 'Test')

grid.arrange(train_plot, val_plot, test_plot, nrow = 1)
```

To extract the background locations we use the [randomPoints](https://www.rdocumentation.org/packages/dismo/versions/1.1-4/topics/randomPoints) function from `dismo` package.

```{r extract background locations}
set.seed(25)
bg_coords <- randomPoints(predictors, 10000)
```

The environmental variables we downloaded have a coarse resolution and the function cannot extract more than `r nrow(bg_coords)` as you can see from the warning message.

Before running the model we have to prepare the data in the correct format. The `prepareSWD` function creates a `SWD` object that stores the species name, the coordinates of the locations and the value of the environmental variables at location points. The parameter `categoricals` indicates which environmental variables are categorical. In our example **biome** is categorical (you can pass a vector if you have more than one categorical environmental variables). The function extracts the value of the environmental variables for each location and excludes those locations that have `NA` value for at least one environmental variable.

```{r prepare SWD datasets}
train <- prepareSWD(species = "Vultur gryphus", coords = train_coords, env = predictors, categoricals = "biome")
val <- prepareSWD(species = "Vultur gryphus", coords = val_coords, env = predictors, categoricals = "biome")
test <- prepareSWD(species = "Vultur gryphus", coords = test_coords, env = predictors, categoricals = "biome")
bg <- prepareSWD(species = "Vultur gryphus", coords = bg_coords, env = predictors, categoricals = "biome")
```

The warning message reports that in the background dataset there are 9 locations that are discarded.  
Let's have a look at the `SWD` object:

```{r show SWD object}
train
```

There are three slots: `species`, `coords` and `data`. To see the data we run:

```{r show train data}
head(train@data)
```

or access the coordinates with `train@coords` or the species with `train@species`.

```{r show train coords data}
head(train@coords)
```

We can save the `SWD` object in a **.csv** file to use it directly in the software **MaxEnt**:
```{r save SWD, eval=F}
swd2csv(train, file_name = "train.csv")
```

We use a sub sample of the background locations to train the model and the full dataset to compute the correlation between the environmental variables. We extract the sub sample using the `getSubsample` function:

```{r get bg subsample}
bg_model <- getSubsample(bg, 5000, seed = 25)
```

and plot the retained background locations:

```{r, fig.align="center"}
ggplot(map_data('world'), aes(long, lat)) +
    geom_polygon(aes(group = group), fill = "grey95", color = "gray40", size = 0.2) +
    geom_jitter(data = bg_model@coords, aes(x = X, y = Y),
                color = 'blue', alpha = 0.4, size = 0.5) +
    labs(x = "longitude", y = "latitude") +
    theme_minimal() +
    theme(legend.position = "none") +
    coord_fixed() +
    scale_x_continuous(limits = c(-125, -32)) +
    scale_y_continuous(limits = c(-56, 40))
```

# Train MaxEnt models

We use the `train` function to train the model. The `method` parameter is used to set which type of model we want to train, in our case a **Maxent** (note that `rm = 1`, `fc = "lqph"` and `iter= 500` are the default settings of MaxEnt software v. 1.4.1 when you have at least 80 presence locations):

```{r run MaxEnt, eval=F}
base_model <- train(method = "Maxent", presence = train, bg = bg_model, rm = 1, fc = "lqph", iter = 500)
```

The function runs Maxent model with a minimum setting of optional parameters (e.g. it doesn't make the response curves) to speed up the computation time. By default the function uses **extra_args = c("noaddsamplestobackground", "removeduplicates=false")**. In case this is not your expected beaviour you can remove both passing **extra_args = ""** or you can add any other additional arguments extending the previous vector.
The function takes care of the environmental variables data type: when we prepared the data we defined **biome** as categorical variable and the information is passed to the `train` function.

The result of the `train` function is an object of class `SDMmodel` that contains in different slots the **presence** and **background** locations and the model itself:

```{r}
base_model
```

The model slot contains an object of class `Maxent` that contains the MaxEnt **results**, **lambdas** coefficients, **rm**, **fc**, **iterations**.

```{r}
base_model@model
```

We can access the slots using `base_model@model@name_of_the_slot` (e.g. `base_model@model@results`).


## Variable Importance

We can see the **percent contribution** and the **permutation importance** of each environmental variable using `maxentVarImp`:

```{r variable contribution}
maxentVarImp(base_model)
```

The previous function returns the valus produced by MaxEnt java software. The estimation of the prmutation importance is computes using only one permutation and the result depends on the seed set for the random permutation. The function `varImp` allows to run a defined number of permutation and averages the results to obtain a better estimate of the permutation importance:

```{r permutation importance}
df <- varImp(base_model)
df
```

By default the function performs 10 permuations but you can set the number of permutation using `permut = 5`. We can plot it using the `plotVarImp` function (the default color is grey):

```{r plot model, fig.align="center"}
plotVarImp(df, color = "#159957")
```

Another way to estimate the variable importance is to run a **Jackknife Test**:

```{r jk, message=F}
jk <- doJk(base_model, with_only = T, metric = "auc", test = val)
```

If the model has a test dataset, it returns the train and test AUC, if not it returns only the train AUC
```{r show jk}
jk
```

We can plot the result of the **Jackknife Test** with the `plotJk` function:

```{r plot jk, fig.align="center"}
plotJk(jk, type = "train", ref = auc(base_model))
```

The red dashed vertical line represents the AUC value of the model trained using all the environmental variables.

## Plot Response Curves

The function `plotResponse` plots the response curve of the given environmental variable:

```{r bio12, fig.align="center"}
plotResponse(base_model, var = 'bio1', type = "cloglog", marginal = T, fun = mean, rug = T, clamp = T)
```

**bio1** is a continuous variable, **biome** is categorical (in the next graph we plot the univariate response curve of the **logistic** output nstead of the  **cloglog** output):

```{r biome, fig.align="center"}
plotResponse(base_model, var = 'biome', type = "logistic", marginal = F, color = '#159957', clamp = T)
```

`color` allows to change the **color** of the plot and `marginal` to plot the marginal response curve.

## Plot ROC curve

The `plotROC` function plots the ROC curve and display the value of the AUC. Is possible to plot the Train ROC curve alone or together with the validation and/or the test one:

```{r plot ROC, fig.align="center"}
plotROC(base_model, val = val)
```


# Make Prediction

The function `trainMaxent` trains a model but doesn't make any prediction. The `predict` method makes the prediction:

```{r make prediction, eval=F}
prediction <- predict(base_model, data = predictors, type = "cloglog", filename = "base_model_prediction", format = "GTiff", overwrite = T)
```
```{r, echo=F}
prediction <- predict(base_model, data = predictors, type = "cloglog")
```

You can pass as `data` a `data frame`, a `SWD object` or a [Raster Stack](https://www.rdocumentation.org/packages/raster/versions/2.6-7/topics/stack), [Raster Brick](https://www.rdocumentation.org/packages/raster/versions/2.6-7/topics/brick) object. In the first case the function returns a vector of prediction, in the second case it returns a raster map that can be saved providing the `filename` parameter. For big dataset you can provide the parameter `progress = "text"` to display a progress bar and take advantage of the parallel computation using `parallel = T`.

## Plot Prediction

The prediction can be plotted using any of the `plot` function in R but the function `plotPred` provides a way to plot it using the [rasterVis](https://oscarperpinan.github.io/rastervis/) package (`lt` stands for "legend title"):

```{r plot prediction, fig.align="center"}
plotPred(prediction, lt = "cloglog output")
```

The default colorramp is similar to the usual MaxEnt output, but you can also use different colorramps:

```{r plot prediction colorramp, fig.align="center"}
plotPred(prediction, colorramp = c("#2c7bb6", "#abd9e9", "#ffffbf", "#fdae61", "#d7191c"), lt = "cloglog output")
```

To get nice combinations of colors for maps visit [COLORBREWER](http://colorbrewer2.org/), the one in this example is the **5-class RdYlBu** colorblind safe and print friendly.
Using `hr = T` you obtain a high resolution plot, useful for publications.

## Plot Presence Absence

To create a presence absence map we choose a threshold provided by MaxEnt. The function `maxentTh` provides a list of possible thresholds:

```{r threshold}
maxentTh(base_model)
```

For example we use the **Maximum training sensitivity plus specificity Cloglog threshold** as parameter for the `plotPA` function:

```{r plot presence absence, eval=F}
plotPA(prediction, th = 0.1169, filename = 'pa_map', overwrite = TRUE, format = 'GTiff')
```
```{r, fig.align="center", echo=F}
plotPA(prediction, th = 0.1169)
```

or we can customise the colors using:

```{r plot presence absence colors, fig.align="center"}
plotPA(prediction, th = 0.1169, colors = c("#7fbf7b", "#af8dc3"))
```

# Data Driven Variable Selection

Some of the environmental variables used to train our **base_model** are highly correlated.
There are two functions that help to visualize if the environmental variables are highly correlated: `plotCor` and `corVar`.
`plotCor` builds a correlation matrix heat map (we use all the extracted background points to checks for autocorrelation):

```{r heat map, fig.align="center"}
plotCor(bg, method = "spearman", cor_th = 0.75)
```

Using the parameter `cor_th` the function prints only the values whose absolute values are higher than the given threshold (in this case 0.75).
`corVar` instead creates a data frame with the correlated variables:

```{r varCor}
corVar(bg, method = "spearman", cor_th = 0.75)
```

One possible approach to remove highly correlated variables is to choose a subset of variables based on expert knowledge of the target species. If you don't know which variable to retain, you can use some kind of data-driven variable selection. The function `varSel` iterates through several steps: starting from a trained model it checks if the variable ranked as most important (both variable importance ranks are possible: percent contribution and permutation importance) is highly correlated with other variables according to the given method and correlation threshold. In this case it performs a Jaccknife test and among the correlated variables it discards the one that decreases the less the model performance. After it trains another model without the removed variable and checks again for highly correlated variables. The process is repeated until the retained variables are not highly correlated anymore. We suggest to use a very low regularization parameter to perform variable selection, in this case we use 0.001 (see Vignali et al. we add here the publication)

```{r variable selection, message=F}
selected_var_model <- varSel(base_model, bg, metric = "auc", rm = 0.001, method = "spearman", cor_th = 0.75)
```

The function returns the model trained without the correlated variables, if we print the object we see that the function removed **bio6**, **bio7**, **bio8** and **bio16**:

```{r}
selected_var_model
```

If we plot the ROC curve we see that after removing the correlated variables the model has still very good performances:

```{r, fig.align="center"}
plotROC(selected_var_model, val = val)
```


# Tune model hyperparameters

`SDMsel` provides a family of functions to tune model hyperparameters. For `Maxent` models we can tune the **feature combination**, the number of **background locations**, the **regularization multiplier** and the **number of iterations**. All the functions of this family return an object of class `SDMtune` with two slots, one for the results and one with all the trained models.

## Tune feature combination

We can test the effect of different feature combinations on model performance using the function `tuneFC`:

```{r, message=F}
tune_fc <- tuneFC(selected_var_model, fcs = c("l", "lq", "lqp", "lqh", "lqph", "lqpht"), metric = "auc")
```

We can display the results with:
```{r}
tune_fc@results
```

or plot them:

```{r, fig.align="center"}
plot_ly(tune_fc@results) %>%
  add_trace(x = ~fc, y = ~train_AUC, type = "bar", name = "Train AUC", marker = list(color = "#00BFC4"), text = ~paste("Diff AUC:", diff_AUC)) %>%
  add_trace(x = ~fc, y = ~test_AUC, type = "bar", name = "Val AUC", marker = list(color = "#F8766D")) %>%
  layout(xaxis = list(title = "Feature Class Combination"), yaxis = list(title = "AUC"), barmode = "group", title = "Model performance", legend = list(orientation = 'h'))
```

We want to maximize the AUC in the validation dataset but we want to acccount also for the difference between train AUC and validation AUC. If you hover hover the previous output and you select "compare data on hover" you notice that **lqph** provides the best result, we select this model from the models slot and use it to tune the number of background locations.

```{r}
tuned_fc <- tune_fc@models[[5]]
```

## Tune number of background locations

The model we trained before used 5000 background locations. To test the effect of using different number of background locations on the model performance we can run the function `tuneBg`. We have a maximum of `r nrow(bg@data)` locations in the `bg` object, in the next example we train a sequence of models using `r c(seq(3000, 9000, 1000), nrow(bg@data))`:

```{r, message=F}
tune_bg <- tuneBg(tuned_fc, bg4test = bg, bgs = c(seq(1000, 9000, 1000), nrow(bg@data)), metric = "auc", seed = 25)
```

Output:

```{r}
tune_bg@results
```

```{r, fig.align="center"}
plot_ly(tune_bg@results) %>%
  add_trace(x = ~bg, y = ~train_AUC, type = "scatter", mode = "lines+markers", name = "Train AUC", marker = list(color = "#00BFC4")) %>%
  add_trace(x = ~bg, y = ~test_AUC, type = "scatter", mode = "lines+markers", name = "Val AUC", marker = list(color = "#F8766D")) %>%
  layout(xaxis = list(title = "Background locations"), yaxis = list(title = "AUC"), title = "Model performance", legend = list(orientation = 'h'))
```

We obtain the best result using all the background locations, we select this model from the models slot and use it to tune the regularization multiplier.

```{r}
tuned_bg <- tune_bg@models[[10]]
```

## Tune Regularization Multiplier

In the next step we tune the `regularization multiplier`. The default regularization multiplier used by MaxEnt is 1, but is not always the best choice. In the next example we test a sequence of regularization multipliers starting from 0.1 till 2.0 with increments of 0.1:

```{r tune rm, message=F}
tune_rm <- tuneRM(tuned_bg, rms = seq(0.1, 2, 0.1), metric = "auc")
```

Output:

```{r show results tuneRM}
tune_rm@results
```

```{r, fig.align="center"}
plot_ly(tune_rm@results) %>%
  add_trace(x = ~rm, y = ~train_AUC, type = "scatter", mode = "lines+markers", name = "Train AUC", marker = list(color = "#00BFC4")) %>%
  add_trace(x = ~rm, y = ~test_AUC, type = "scatter", mode = "lines+markers", name = "Val AUC", marker = list(color = "#F8766D")) %>%
  layout(xaxis = list(title = "Regularization multiplier"), yaxis = list(title = "AUC"), title = "Model performance", legend = list(orientation = 'h'))
```

The model with the highest validation AUC uses a `regularization multiplier` equal to 0.5 but at rm = 0.9 we have a lower differece between train and validation AUC with an irrelevant decrease of validation AUC, we select this model from the models slot and use it to tune the number of iterations.

```{r}
tuned_rm <- tune_rm@models[[9]]
```

## Tune number of iterations:

The default number of iterations used by the optimization algorithm in MaxEnt is 500. This value is in general good but can be tuned like other hyperparameters. We test a sequence iterations starting from 400 till 2300 with increments of 100:

```{r tune it, message=F}
tune_it <- tuneIt(tuned_rm, its = seq(400, 1100, 50), metric = "auc")
```

Output:

```{r}
tune_it@results
```

```{r, fig.align="center"}
plot_ly(tune_it@results) %>%
  add_trace(x = ~it, y = ~train_AUC, type = "scatter", mode = "lines+markers", name = "Train AUC", marker = list(color = "#00BFC4")) %>%
  add_trace(x = ~it, y = ~test_AUC, type = "scatter", mode = "lines+markers", name = "Val AUC", marker = list(color = "#F8766D")) %>%
  layout(xaxis = list(title = "Number of iterations"), yaxis = list(title = "AUC"), title = "Model performance", legend = list(orientation = 'h'))
  
```

The AUC changes varying the number of iterations in both Train and Validation datasets. In this case there are not much differences with the default value of 500 iterations (this is not always the case) but using 1050 iterations we obtain a lower difference between Train and Validation AUC, we select this model from the models slot:

```{r}
tuned_it <- tune_it@models[[which.min(tune_it@results$diff_AUC)]]
```

## Remove less important variables

Let's say that we would like to remove variables with a contribution (permutation or percent) lower than a given threshold. We could simply remove them in a stepwise fashion untill the variable with the lower contribution contributes more than the given threshold or we could remove the variable only if the model performace doesn't decrease. The function `reduceVar` performs both approaches, in the next example we demonstrate the second approach.

We print the variable contribution:

```{r}
varImp(tuned_it)
```

We decide we want to remove the variables with a permutation importance lower than 3\% but we do it only if the model performance doesn't decrease and we use the **AUC** of the Validation dataset to measure the model performance:

```{r}
tuned_model <- reduceVar(tuned_it, th = 3, metric = "auc", use_jk = T)
```

We print again the variable contribution:

```{r}
varImp(tuned_model)
```

but **bio17** was not removed because the model without bio17 has lower performance than the one with. We can demonstrate it removing the variable and comparing the **AUC** values on the Validation dataset:

```{r}
model_without_bio17 <- reduceVar(tuned_it, th = 3)
varImp(model_without_bio17)
```

Compare AUC:

```{r}
cat("Validation AUC with bio17   : ", auc(tuned_it, val),
    "\nValidation AUC without bio17: ", auc(model_without_bio17, val))
```


# Tuned Model

We can see the final setting of the tuned model printing the model object:

```{r}
tuned_model
```

All the functions of the tuning family don't save the MaxEnt ouput but we could save it using the `trainMaxent` function and setting all the hyperparameter values according with the previous output. The `maxentOutput` function makes this step easier, we just have to pass the model and it will set the hyperparameters for us. In addition there are other parameters to add the response curves and the Jackknife test to the html output:

```{r, eval=F}
modelOutput(tuned_model, response_curves = T, jk = T, threads = 4, folder = "final_output")
```

At the end of the tuning process is time to use the **Test** dataset to have an unbiased estimate of the model performance. We use the `plotROC` function for that:

```{r, fig.align="center"}
plotROC(tuned_model, val = val, test = test)
```


# References
