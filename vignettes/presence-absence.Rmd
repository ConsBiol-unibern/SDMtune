---
title: "SDMtune - presence absence models"
author: "Sergio Vignali, Arnaud Barras & Veronika Braunisch"
bibliography: SDMtune.bib
output:
  html_vignette:
    toc: yes
    toc_depth: 2
vignette: >
 %\VignetteIndexEntry{SDMtune - presence absence models} 
 %\VignetteEncoding{UTF-8}
 %\VignetteEngine{knitr::rmarkdown}
---

```{r, include = FALSE}
knitr::opts_chunk$set(comment = "#>", collapse = TRUE, eval = FALSE,
                      fig.align = "center")
```

<img src="../man/figures/logo.png" width="70" style = "float: right; border: none;">

The other vignettes are based on presence only methods. Here you will learn how to train a presence absence model. The following examples are based on the Artificial Neural Networks method [@Venables2002], but you can adapt the code for any of the other supported methods. We use the first 8 environmental variables and the `virtualSp` dataset selecting the absence instead of the background locations.
```{r load data}
library(SDMtune)
library(zeallot)

# Prepare data
files <- list.files(path = file.path(system.file(package = "dismo"), "ex"),
                    pattern = "grd",
                    full.names = TRUE)

predictors <- terra::rast(files)
p_coords <- virtualSp$presence
a_coords <- virtualSp$absence
data <- prepareSWD(species = "Virtual species",
                   p = p_coords,
                   a = a_coords,
                   env = predictors[[1:8]])

# Split data in training and testing datasets
c(train, test) %<-% trainValTest(data,
                                 test = 0.2,
                                 seed = 25)

cat("# Training  : ", nrow(train@data))
cat("\n# Testing   : ", nrow(test@data))

# Create folds
folds <- randomFolds(train,
                     k = 4,
                     seed = 25)
```

# Train the model

We first train the model with default settings and using 10 neurons:
```{r}
set.seed(25)
model <- train("ANN",
               data = train,
               size = 10,
               folds = folds)

model
```

Let's check the training and testing AUC:
```{r auc}
auc(model)
auc(model, test = TRUE)
```

# Tune model hyperparameters

To check which hyperparameters can be tuned we use the function `getTunableArgs` function:
```{r get tunable args}
getTunableArgs(model)
```

We use the function `optimizeModel` to tune the hyperparameters:

```{r optimize model}
h <- list(size = 10:50,
          decay = c(0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5),
          maxit = c(50, 100, 300, 500))

om <- optimizeModel(model,
                    hypers = h,
                    metric = "auc",
                    seed = 25)
```

The best model is:
```{r best model}
best_model <- om@models[[1]]
om@results[1, ]
```

# Evaluate the final model

We now train a model with the same configuration as found by the function`optimizeModel`, without cross validation, using all the train data, and we evaluate it using the held apart testing dataset:
```{r evaluate final model, fig.align="center"}
set.seed(25)
final_model <- train("ANN",
                     data = train,
                     size = om@results[1, 1],
                     decay = om@results[1, 2],
                     maxit = om@results[1, 4])

plotROC(final_model,
        test = test)
```

# References
